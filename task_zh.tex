\section{进程管理}

\subsection{进程控制块}
\lstinline{struct task_struct} 是Linux内核的进程控制块（PCB）.
它存储进程的标识符、进程的状态、指向存储进程上下文的数据结构的指针、调度所需的信息以及与进程相关的资源和指向其他 \lstinline{task_struct} 的指针等.
所有与进程有关的操作都会直接或间接地修改进程控制块来达到目的.
下面，我们介绍管理进程所需要的多种信息，并介绍它们是如何在 \lstinline{task_struct} 中表示的.

\begin{readsrcbox}{进程控制块}
	\lstinline{struct task_struct} 是在\lstinline{include/linux/sched.h}中定义的.
\end{readsrcbox}

\subsubsection{进程的状态}
首先是进程的状态，一个进程可能处于这些状态：
\begin{itemize}
	\item \lstinline{TASK_NEW}.
	      当一个进程刚创建时，它的状态就被设置为\lstinline{TASK_NEW}，表示这个进程已经被创建，但是还没有开始运行.
	\item \lstinline{TASK_DEAD}.
	      进程已经结束.
	\item \lstinline{TASK_RUNNING}.
	      进程正在运行，即该进程在正在运行的进程的队列里.
	\item \lstinline{TASK_INTERRUPTIBLE} 或者 \lstinline{TASK_UNINTERRUPTIBLE}.
	      处于这两个状态的进程正在等待，但是只有 \lstinline{TASK_INTERRUPTIBLE} 状态的进程才会被信号唤醒.
	\item \lstinline{TASK_NOLOAD}.
	      类似 \lstinline{TASK_UNINTERUPTIBLE} 但是在统计数据中不被计入负载.
	      \footnote{\url{https://lore.kernel.org/lkml/alpine.LFD.2.11.1505112154420.1749@ja.home.ssi.bg/T/}}
	\item \lstinline{TASK_WAKING}.
	      进程已经被要求唤醒，但是还没有进入正在运行的进程队列.
	      \footnote{\url{https://lore.kernel.org/lkml/tip-e9c8431185d6c406887190519f6dbdd112641686@git.kernel.org/}}
	\item \lstinline{TASK_WAKEKILL}.
	      该进程收到SIGKILL信号时会被唤醒.
	\item \lstinline{TASK_TRACED}.
	      调试器暂停该进程来追踪它的运行状态.
\end{itemize}
这些状态被编码成掩码，读写进程的状态时，
需要用这些掩码操作 \lstinline{task_struct} 的 \lstinline{__state} 域.
比如识别一个进程是否处于某状态，
就要用它的 \lstinline{__state} 和 这个状态的掩码做与操作，
若结果为0则不处于这个状态.
这些状态中有的状态可以组合成新的掩码，方便使用.

\subsubsection{进程的上下文}
进程控制块还需要存储有关进程上下文的信息，例如有关栈和堆的信息和CPU内部寄存器的值.
这一部分信息既和内核的内存管理有关又依赖于具体的硬件架构，
是实现进程调度中挂起和重启进程所需要的数据结构.
\begin{readsrcbox}{架构相关代码}
	为了提高可移植性，Linux内核的代码区分不依赖于具体硬件架构的代码和针对特定架构的代码.
	架构相关的代码全部在 \lstinline{arch} 目录下.
	例如 x86和x86\_64的代码位于 \lstinline{arch/x86}.

	显然，所有的汇编代码都应该放在该目录.
	内存管理和进程管理所需的某些功能也依赖于CPU的特定功能，
	需要根据硬件功能定义数据结构和执行具体的指令，
	这些定义和实现也位于 \lstinline{arch} 下具体架构的目录下.
	不同的架构的代码尽量暴露出相同的接口，供架构无关代码使用.

	某一架构上的具体实现的文档可以在 \href{https://docs.kernel.org/arch.html}{\lstinline{Document/arch.rst}} 中的列表找到.
	例如\href{https://docs.kernel.org/x86/kernel-stacks.html}{\lstinline{Document/x86/kernel-stacks.rst}}介绍了x86\_64 CPU上内核为每一个进程维护的若干个栈.
\end{readsrcbox}
\lstinline{task_struct}的定义中，有一个 \lstinline{void *stack;} 域.
这不是该进程的用户态的栈的起始地址，
而是该进程的\textbf{内核栈}的起始地址，每次该进程通过系统调用从用户态进入内核态时，
内核中的代码都在这个内核栈上执行.

\lstinline{tast_struct}的最后一个成员是 \lstinline{struct thread_struct thread;}，
进程在CPU上的运行状态存放在这个架构相关的结构体中.
x86\_64上该结构体的大小可以变化，因此它必须作为\lstinline{tast_struct}的最后一个成员.
这个 \lstinline{struct thread_struct} 的成员直接或间接地保存该进程的上下文.

我们在微机原理中学过x86\_32架构对于上下文切换有专门的硬件支持.
Global Descriptor Table（GDT）中有专门的描述符指向Task State Segment（TSS）.
TSS可以用来存储所有的x86寄存器，
在需要时自动进行硬件上下文切换，把寄存器的值存储在旧的TSS中，然后把新TSS的内容加载到寄存器中.
x86\_64取消了硬件上下文切换的功能，但即使是在32位x86机器上，
Linux内核也不是用TSS来进行硬件切换，
因为考虑到其它架构几乎都不支持这一机制，使用软件实现上下文切换可移植性更好.

Linux 内核在上下文切换时，寄存器的值实际上是存在进程的内核栈上的.
进程在切换之前，一定正在该进程的内核栈上执行内核态的代码，
这时用户程序的寄存器已经在进入内核态后保存下来了，
要保存运行在内核态时的寄存器，只需要 \lstinline{pushq} 所有的\textit{被调用者保存}寄存器即可.
这是因为在进入当前栈帧前后，调用者一定已经保存了其所需的\textit{调用者保存}寄存器.
返回地址也在栈帧中有记录.
既然这个栈帧就记录着当前进程的上下文，PCB只需保存这个栈帧的地址即可.
因此 \lstinline{thread_struct} 有一个 \lstinline{unsigned long sp;} 域，
存放的就是切换前的寄存器sp的值，即这个栈帧的地址.

除了整数运算的寄存器，浮点和向量运算的寄存器也需要保存，
但是由于浮点和向量运算不像整数运算那样常见，
且保存这些状态有一定开销，
之前的Linux内核只在这些运算真正执行过时才保存对应的寄存器.
这种“懒惰”的保存方式是通过CPU的硬件支持完成的：
每次上下文切换时，CPU的TS标志位就被置为1；
每次浮点或向量运算指令执行时，若TS标志位为1，则会报“Device Not Available”异常.
内核在处理这个异常时会记录下来：该进程使用了浮点或向量运算.
再次切换进程时，若发现使用了浮点或向量运算，则保存相应的状态.\cite{bovet2005understanding}
而现代的CPU引入了特定的指令来保存FPU状态，
减小了保存FPU状态的开销，
而且程序越来越依赖向量指令来进行拷贝数据等操作，
所以Linux在2016年取消了“懒惰”的保存方式，
\footnote{\url{https://lore.kernel.org/lkml/e3b2baadcd19bf8abcd3bcd60d19e8e50e75f63a.1453510332.git.luto@kernel.org/}}
每次进程切换都保存FPU的状态.
除了进程切换，
另外一个需要保存和恢复FPU状态的时机是内核代码在用户上下文中使用浮点或向量运算的前后，
这也是按需进行的，
即返回用户态时，若改变了FPU状态才加载用户的FPU状态.

FPU的状态保存在 \lstinline{thread_struct} 的 \lstinline{struct fpu fpu;}域.
这个数据结构是变长的，这也是 \lstinline{struct thread_struct thread;}
要放在 \lstinline{task_struct} 最后的原因.

\begin{readsrcbox}{\lstinline{thread_struct}}
	\lstinline{thread_struct} 是架构相关的.
	x86的 \lstinline{thread_struct} 的定义在 \lstinline{arch/x86/include/asm/processor.h} 中.

	进程调度总是调用通过一个名为 \lstinline{schedule()}
	（\lstinline{kernel/sched/core.c}） 的函数实现的.
	真正完成调度的主要函数为 \lstinline{__schedule()}，
	其上方的注释说明了所有会调用它的情况.

	而最终完成进程切换的是一个名为 \lstinline{switch_to(prev, next, last)} 的宏
	（x86上的定义在 \lstinline{arch/x86/include/asm/switch_to.h}，
	64位机器的实现，汇编部分在 \lstinline{arch/x86/entry/entry_64.S}.
	C语言部分在 \lstinline{arch/x86/kernel/process_64.c}）.
	其关键的操作就是切换进程的内核栈：
	\begin{enumerate}
		\item 将被调用者保存寄存器压入栈；
		\item 把rsp寄存器存入原来进程的 \lstinline{thread->sp}；
		\item 把新进程的 \lstinline{thread->sp} 放入rsp寄存器；
		\item 最后从新的栈中弹出所有调用者保存寄存器.
	\end{enumerate}
\end{readsrcbox}

% TODO: figure

\begin{qbox}{\lstinline{struct thread_info} 与 PCB有什么关系呢？}
	\lstinline{struct task_struct}是PCB，
	而 \lstinline{struct thread_info} 存放的是需要在汇编代码中直接使用的底层的进程信息.
	\lstinline{struct thread_info}的大小被控制在一个cache行内，这是由于这些底层信息需要被频繁查看.
	为了尽量利用空间，信息都按位编码在其中.
	例如，上文提到的“内核使用了浮点或向量运算而需要加载原来的FPU状态”，
	就存储在 \lstinline{thread_info} 的第 \lstinline{TIF_NEED_FPU_LOAD} 位.

	\lstinline{struct thread_info} 在更早的Linux版本中位于进程的内核栈的起始位置
	\cite{bovet2005understanding}，
	可能造成一些疑惑.
	现在的x86 Linux内核默认把 \lstinline{struct thread_info} 放入
	\lstinline{struct task_struct}，
	与PCB的其他成员处于类似的地位.
	\footnote{\url{https://lore.kernel.org/all/cover.1473801993.git.luto@kernel.org/}}
	\footnote{\url{https://lore.kernel.org/all/a50eab40abeaec9cb9a9e3cbdeafd32190206654.1473801993.git.luto@kernel.org}}
\end{qbox}

Scheduling information is also included in the PCB, including priority of the process, schedule entity that represents the process and a pointer to the schedule class that this process is using.
